"""
Script para descargar artÃ­culos de Wikipedia en inglÃ©s y espaÃ±ol.
Genera corpus de texto moderno para entrenamiento de modelos n-gram.
"""

import wikipediaapi
import re
import os
import time
import random
from pathlib import Path

# CategorÃ­as variadas para obtener vocabulario diverso
CATEGORIES = {
    'en': [
        # TecnologÃ­a
        'Computer_science', 'Artificial_intelligence', 'Internet', 'Software',
        'Mobile_phones', 'Social_media', 'Cryptocurrency', 'Video_games',
        # Ciencia
        'Physics', 'Biology', 'Chemistry', 'Medicine', 'Climate_change',
        'Space_exploration', 'Genetics', 'Psychology',
        # Deportes
        'Association_football', 'Basketball', 'Tennis', 'Olympic_Games',
        'American_football', 'Baseball', 'Swimming_(sport)',
        # Cultura
        'Cinema', 'Television', 'Music', 'Literature', 'Art',
        'Photography', 'Fashion', 'Cooking',
        # Sociedad
        'Economics', 'Politics', 'Education', 'Health', 'Environment',
        'Tourism', 'Transportation', 'Architecture',
        # Vida cotidiana
        'Food', 'Clothing', 'Family', 'Housing', 'Employment',
    ],
    'es': [
        # TecnologÃ­a
        'InformÃ¡tica', 'Inteligencia_artificial', 'Internet', 'Software',
        'TelÃ©fono_mÃ³vil', 'Redes_sociales', 'Criptomoneda', 'Videojuegos',
        # Ciencia
        'FÃ­sica', 'BiologÃ­a', 'QuÃ­mica', 'Medicina', 'Cambio_climÃ¡tico',
        'ExploraciÃ³n_espacial', 'GenÃ©tica', 'PsicologÃ­a',
        # Deportes
        'FÃºtbol', 'Baloncesto', 'Tenis', 'Juegos_OlÃ­mpicos',
        'FÃºtbol_americano', 'BÃ©isbol', 'NataciÃ³n',
        # Cultura
        'Cine', 'TelevisiÃ³n', 'MÃºsica', 'Literatura', 'Arte',
        'FotografÃ­a', 'Moda', 'GastronomÃ­a',
        # Sociedad
        'EconomÃ­a', 'PolÃ­tica', 'EducaciÃ³n', 'Salud', 'Medio_ambiente',
        'Turismo', 'Transporte', 'Arquitectura',
        # Vida cotidiana
        'Alimento', 'Ropa', 'Familia', 'Vivienda', 'Empleo',
    ]
}

# ArtÃ­culos populares/importantes adicionales para garantizar contenido
SEED_ARTICLES = {
    'en': [
        'United_States', 'World_War_II', 'Climate_change', 'COVID-19_pandemic',
        'Artificial_intelligence', 'Internet', 'Computer', 'Facebook',
        'Google', 'Apple_Inc.', 'Microsoft', 'Amazon_(company)',
        'Football', 'Basketball', 'Olympic_Games', 'FIFA_World_Cup',
        'New_York_City', 'London', 'Tokyo', 'Paris',
        'Music', 'Film', 'Television', 'Video_game',
        'Science', 'Technology', 'Engineering', 'Mathematics',
        'History', 'Geography', 'Philosophy', 'Psychology',
        'Democracy', 'Human_rights', 'Climate', 'Environment',
        'Health', 'Medicine', 'Education', 'Economy',
        'Food', 'Water', 'Energy', 'Transportation',
        'Communication', 'Social_media', 'Smartphone', 'Email',
    ],
    'es': [
        'Estados_Unidos', 'Segunda_Guerra_Mundial', 'Cambio_climÃ¡tico', 'Pandemia_de_COVID-19',
        'Inteligencia_artificial', 'Internet', 'Computadora', 'Facebook',
        'Google', 'Apple_Inc.', 'Microsoft', 'Amazon',
        'FÃºtbol', 'Baloncesto', 'Juegos_OlÃ­mpicos', 'Copa_Mundial_de_FÃºtbol',
        'Ciudad_de_MÃ©xico', 'Madrid', 'Buenos_Aires', 'Barcelona',
        'MÃºsica', 'Cine', 'TelevisiÃ³n', 'Videojuego',
        'Ciencia', 'TecnologÃ­a', 'IngenierÃ­a', 'MatemÃ¡ticas',
        'Historia', 'GeografÃ­a', 'FilosofÃ­a', 'PsicologÃ­a',
        'Democracia', 'Derechos_humanos', 'Clima', 'Medio_ambiente',
        'Salud', 'Medicina', 'EducaciÃ³n', 'EconomÃ­a',
        'AlimentaciÃ³n', 'Agua', 'EnergÃ­a', 'Transporte',
        'ComunicaciÃ³n', 'Red_social', 'TelÃ©fono_inteligente', 'Correo_electrÃ³nico',
    ]
}


def clean_text(text: str) -> str:
    """
    Limpia el texto de Wikipedia removiendo elementos no deseados.
    """
    # Remover referencias [1], [2], etc.
    text = re.sub(r'\[\d+\]', '', text)
    
    # Remover URLs
    text = re.sub(r'http[s]?://\S+', '', text)
    
    # Remover caracteres especiales pero mantener puntuaciÃ³n bÃ¡sica
    text = re.sub(r'[^\w\s\.\,\;\:\!\?\'\"\-\(\)Ã¡Ã©Ã­Ã³ÃºÃ¼Ã±ÃÃ‰ÃÃ“ÃšÃœÃ‘]', ' ', text)
    
    # Normalizar espacios mÃºltiples
    text = re.sub(r'\s+', ' ', text)
    
    # Remover lÃ­neas muy cortas (probablemente encabezados)
    lines = text.split('\n')
    lines = [line.strip() for line in lines if len(line.strip()) > 50]
    
    return '\n'.join(lines)


def get_article_text(wiki: wikipediaapi.Wikipedia, title: str) -> str:
    """
    Obtiene el texto de un artÃ­culo de Wikipedia.
    """
    page = wiki.page(title)
    
    if not page.exists():
        return ""
    
    return page.text


def get_category_articles(wiki: wikipediaapi.Wikipedia, category_name: str, max_articles: int = 20) -> list:
    """
    Obtiene artÃ­culos de una categorÃ­a de Wikipedia.
    """
    cat = wiki.page(f"Category:{category_name}")
    
    if not cat.exists():
        return []
    
    articles = []
    for member in cat.categorymembers.values():
        if member.ns == wikipediaapi.Namespace.MAIN:  # Solo artÃ­culos principales
            articles.append(member.title)
            if len(articles) >= max_articles:
                break
    
    return articles


def download_wikipedia_corpus(lang: str, output_path: str, target_size_mb: float = 5.0):
    """
    Descarga un corpus de Wikipedia para un idioma especÃ­fico.
    
    Args:
        lang: CÃ³digo de idioma ('en' o 'es')
        output_path: Ruta del archivo de salida
        target_size_mb: TamaÃ±o objetivo en MB
    """
    print(f"\n{'='*60}")
    print(f"Descargando corpus de Wikipedia ({lang.upper()})")
    print(f"{'='*60}")
    
    wiki = wikipediaapi.Wikipedia(
        user_agent='LanguageDetectorTrainer/1.0 (training data collection)',
        language=lang
    )
    
    target_bytes = int(target_size_mb * 1024 * 1024)
    collected_text = []
    collected_bytes = 0
    processed_titles = set()
    
    # Primero, procesar artÃ­culos semilla (garantizados)
    print(f"\nðŸ“– Procesando artÃ­culos semilla...")
    seed_articles = SEED_ARTICLES.get(lang, [])
    
    for title in seed_articles:
        if collected_bytes >= target_bytes:
            break
            
        if title in processed_titles:
            continue
            
        text = get_article_text(wiki, title)
        if text:
            cleaned = clean_text(text)
            if len(cleaned) > 500:  # Solo textos sustanciales
                collected_text.append(cleaned)
                collected_bytes += len(cleaned.encode('utf-8'))
                processed_titles.add(title)
                print(f"  âœ“ {title} ({len(cleaned):,} chars)")
        
        time.sleep(0.1)  # Rate limiting
    
    print(f"\n  Progreso: {collected_bytes/1024/1024:.2f} MB / {target_size_mb} MB")
    
    # Luego, procesar categorÃ­as
    print(f"\nðŸ“š Procesando categorÃ­as...")
    categories = CATEGORIES.get(lang, [])
    random.shuffle(categories)  # Aleatorizar para variedad
    
    for category in categories:
        if collected_bytes >= target_bytes:
            break
            
        print(f"\n  CategorÃ­a: {category}")
        articles = get_category_articles(wiki, category, max_articles=15)
        
        for title in articles:
            if collected_bytes >= target_bytes:
                break
                
            if title in processed_titles:
                continue
                
            text = get_article_text(wiki, title)
            if text:
                cleaned = clean_text(text)
                if len(cleaned) > 500:
                    collected_text.append(cleaned)
                    collected_bytes += len(cleaned.encode('utf-8'))
                    processed_titles.add(title)
                    print(f"    âœ“ {title[:40]}... ({len(cleaned):,} chars)")
            
            time.sleep(0.1)  # Rate limiting
        
        print(f"  Progreso: {collected_bytes/1024/1024:.2f} MB / {target_size_mb} MB")
    
    # Guardar corpus
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    full_text = '\n\n'.join(collected_text)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(full_text)
    
    final_size = len(full_text.encode('utf-8'))
    
    print(f"\n{'='*60}")
    print(f"âœ… Corpus guardado: {output_path}")
    print(f"   ArtÃ­culos: {len(processed_titles)}")
    print(f"   TamaÃ±o: {final_size/1024/1024:.2f} MB")
    print(f"   LÃ­neas: {len(full_text.splitlines()):,}")
    print(f"{'='*60}")
    
    return final_size


def create_balanced_corpus(wiki_path: str, bible_path: str, output_path: str, wiki_ratio: float = 0.9):
    """
    Crea un corpus balanceado combinando Wikipedia y texto bÃ­blico.
    
    Args:
        wiki_path: Ruta al corpus de Wikipedia
        bible_path: Ruta al corpus bÃ­blico
        output_path: Ruta de salida
        wiki_ratio: ProporciÃ³n de Wikipedia (0.9 = 90% wiki, 10% bÃ­blico)
    """
    print(f"\nðŸ“Š Creando corpus balanceado...")
    
    # Leer Wikipedia
    with open(wiki_path, 'r', encoding='utf-8') as f:
        wiki_text = f.read()
    
    # Leer texto bÃ­blico
    with open(bible_path, 'r', encoding='utf-8') as f:
        bible_text = f.read()
    
    wiki_size = len(wiki_text.encode('utf-8'))
    
    # Calcular cuÃ¡nto texto bÃ­blico incluir
    bible_target = int(wiki_size * (1 - wiki_ratio) / wiki_ratio)
    bible_text_trimmed = bible_text[:bible_target] if len(bible_text.encode('utf-8')) > bible_target else bible_text
    
    # Combinar
    combined = wiki_text + '\n\n' + bible_text_trimmed
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(combined)
    
    final_size = len(combined.encode('utf-8'))
    
    print(f"   Wikipedia: {wiki_size/1024/1024:.2f} MB ({wiki_ratio*100:.0f}%)")
    print(f"   BÃ­blico: {len(bible_text_trimmed.encode('utf-8'))/1024/1024:.2f} MB ({(1-wiki_ratio)*100:.0f}%)")
    print(f"   Total: {final_size/1024/1024:.2f} MB")
    print(f"   Guardado en: {output_path}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Descargar corpus de Wikipedia para entrenamiento")
    parser.add_argument("--lang", choices=['en', 'es', 'both'], default='both',
                        help="Idioma a descargar (default: both)")
    parser.add_argument("--size", type=float, default=5.0,
                        help="TamaÃ±o objetivo en MB (default: 5.0)")
    parser.add_argument("--output-dir", default="data",
                        help="Directorio de salida (default: data)")
    parser.add_argument("--create-balanced", action="store_true",
                        help="Crear tambiÃ©n corpus balanceados con texto bÃ­blico")
    
    args = parser.parse_args()
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    if args.lang in ['en', 'both']:
        download_wikipedia_corpus('en', str(output_dir / 'en-wikipedia.txt'), args.size)
        
        if args.create_balanced:
            bible_path = output_dir / 'en-bible.txt'
            if bible_path.exists():
                create_balanced_corpus(
                    str(output_dir / 'en-wikipedia.txt'),
                    str(bible_path),
                    str(output_dir / 'en-final.txt')
                )
    
    if args.lang in ['es', 'both']:
        download_wikipedia_corpus('es', str(output_dir / 'es-wikipedia.txt'), args.size)
        
        if args.create_balanced:
            bible_path = output_dir / 'es-bible.txt'
            if bible_path.exists():
                create_balanced_corpus(
                    str(output_dir / 'es-wikipedia.txt'),
                    str(bible_path),
                    str(output_dir / 'es-final.txt')
                )
    
    print("\nðŸŽ‰ Â¡Descarga completada!")


